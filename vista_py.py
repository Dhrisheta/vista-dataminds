# -*- coding: utf-8 -*-
"""vista.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14uXaO3ULn7HUL5F1bsfe-tpek6ZRu0sd
"""

!pip install opencv-python

!pip install opencv-python yt_dlp numpy roboflow

!pip install yt-dlp

!pip install roboflow

!wget https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg -O /content/yolov3.cfg

!wget https://pjreddie.com/media/files/yolov3.weights -O /content/yolov3.weights

import cv2
import numpy as np
import os
import time
import yt_dlp
from roboflow import Roboflow

# Ensure correct paths for YOLOv3 weights and config
weights_path = "/content/yolov3.weights"
config_path = "/content/yolov3.cfg"

# Load YOLOv3 model
net = cv2.dnn.readNet(weights_path, config_path)
layer_names = net.getLayerNames()
output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]

rf = Roboflow(api_key="ZnJzXaB86Tol6zubMIPd")  # Insert your API key
project = rf.workspace().project("pixelation_mk")  # Insert your project ID or endpoint
model = project.version(8).model  # Replace with the correct version number

# Function to get YouTube stream URL using yt-dlp and cookies
def get_youtube_stream_url(video_url):
    ydl_opts = {
        'format': 'best[ext=mp4]/best',  # Select the best video format with mp4 extension
        'noplaylist': True,              # Ensure only a single video is downloaded
        'cookiefile': 'D:/COLLEGE/SEM 5/LAB/ML/Project/youtube_cookies.txt'  # Path to your cookies file
    }

    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info_dict = ydl.extract_info(video_url, download=False)
        return info_dict.get("url", None)

youtube_video_url = "https://www.youtube.com/watch?v=MUgU-gUOgos&pp=ygUUZ29yZSBzY2VuZXMgaW4gZ2FtZXM%3D"
stream_url = get_youtube_stream_url(youtube_video_url)

cap = cv2.VideoCapture(stream_url)

if not cap.isOpened():
    print("Error: Could not open input video stream.")
    exit()

# Get video properties
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps1 = cap.get(cv2.CAP_PROP_FPS)

# Print video properties for debugging
print(f"Frame Width: {frame_width}, Frame Height: {frame_height}, FPS: {fps1}")

# Video output path
output_video_path = "D:/COLLEGE/SEM 5/LAB/ML/Project/Final/youtube_original.avi"

# Define the codec and create a VideoWriter object
fourcc = cv2.VideoWriter_fourcc(*'MJPG')  # Use MJPG for .avi format
fps2 = 30
out = cv2.VideoWriter(output_video_path, fourcc, fps2, (frame_width, frame_height))

# Check if the VideoWriter was initialized correctly
if not out.isOpened():
    print("Error: Could not open output video file for writing.")
    cap.release()
    exit()

# Pixelation function
def pixelate_region(frame, x, y, w, h):
    roi = frame[y:y+h, x:x+w]
    roi_small = cv2.resize(roi, (10, 10), interpolation=cv2.INTER_LINEAR)
    roi_pixelated = cv2.resize(roi_small, (w, h), interpolation=cv2.INTER_NEAREST)
    frame[y:y+h, x:x+w] = roi_pixelated

# Start processing the video frame by frame
frame_counter = 0
fps_avg = []  # List to store per-frame FPS
total_processing_time = 0

start_time = time.time()  # Start the timer for total processing

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        print("End of video or cannot read the frame.")
        break

    frame_counter += 1
    frame_start_time = time.time()  # Start timer for each frame

    height, width, channels = frame.shape

    # Prepare the frame for YOLOv3 detection
    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)
    net.setInput(blob)
    outputs = net.forward(output_layers)

    # Get the bounding boxes for detected objects (e.g., person)
    boxes = []
    confidences = []
    class_ids = []
    for output in outputs:
        for detection in output:
            scores = detection[5:]
            class_id = np.argmax(scores)
            confidence = scores[class_id]
            if confidence > 0.5:  # confidence threshold
                center_x = int(detection[0] * width)
                center_y = int(detection[1] * height)
                w = int(detection[2] * width)
                h = int(detection[3] * height)
                x = int(center_x - w / 2)
                y = int(center_y - h / 2)

                # Ensure the bounding box is within the frame
                x = max(0, min(x, width - 1))
                y = max(0, min(y, height - 1))
                w = min(w, width - x)
                h = min(h, height - y)

                boxes.append([x, y, w, h])
                confidences.append(float(confidence))
                class_ids.append(class_id)

    # Non-maximum suppression to eliminate overlapping boxes
    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)

    # Check if indices is not empty
    if len(indices) > 0:
        for i in indices:
            # If indices is an array of lists, unpack; otherwise, just use the scalar
            if isinstance(i, (list, np.ndarray)):
                i = i[0]
            box = boxes[i]
            x, y, w, h = box

            # Prepare the region for pixelation using YOLOv8 model
            roi = frame[y:y+h, x:x+w]
            prediction = model.predict(roi, confidence=40, overlap=30).json()
            print(f"Predictions for frame {frame_counter}: {prediction}")

            # Check and process predictions for pixelation
            if 'predictions' in prediction:
                for pred in prediction['predictions']:
                    if pred['class'] == "pixelate":  # Adjust this condition based on your model's output
                        pixelate_region(frame, x, y, w, h)

    # Write the processed frame to the output video
    out.write(frame)

    # Measure time taken for the frame and calculate FPS for the frame
    frame_end_time = time.time()
    frame_processing_time = frame_end_time - frame_start_time
    total_processing_time += frame_processing_time
    fps = 1 / frame_processing_time
    fps_avg.append(fps)

    # Print frame processing details (optional)
    print(f"Processed frame {frame_counter}, Time: {frame_processing_time:.4f} s, FPS: {fps:.2f}")

# Release video capture and writer objects
cap.release()
out.release()

# Print final performance results
print(f"\n--- Video Processing Complete ---")
print(f"Total frames processed: {frame_counter}")
print(f"Total processing time: {total_processing_time:.2f} seconds")
print(f"Average FPS: {sum(fps_avg) / len(fps_avg) if fps_avg else 0:.2f}")

!pip install inference-cli

!inference server start

!inference model download roboflow/violent-gtzs0 --api-key ZnJzXaB86Tol6zubMIPd

!pip install inference-cli

!sudo systemctl start docker

!inference server start

!docker --version

inference model download dhrisheta-ms-wigrz/pixelation_mk --version 8 --api-key ZnJzXaB86Tol6zubMIPd

!pip install opencv-python yt-dlp roboflow numpy

import cv2
import numpy as np
import time
import yt_dlp
from roboflow import Roboflow

# Ensure correct paths for YOLOv3 weights and config
weights_path = "/content/yolov3.weights"
config_path = "/content/yolov3.cfg"

# Load YOLOv3 model
net = cv2.dnn.readNet(weights_path, config_path)
layer_names = net.getLayerNames()
output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]

# Initialize Roboflow workspace and project
rf = Roboflow(api_key="ZnJzXaB86Tol6zubMIPd")  # Insert your API key
project = rf.workspace().project("pixelation_mk")  # Insert your project ID or endpoint
model = project.version(8).model  # Replace with the correct version number

# Function to get YouTube stream URL using yt-dlp
def get_youtube_stream_url(video_url):
    ydl_opts = {
        'format': 'best[ext=mp4]/best',  # Select the best video format with mp4 extension
        'noplaylist': True,              # Ensure only a single video is downloaded
        'cookiefile': None               # Disable cookies to avoid the cookie file issue
    }

    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info_dict = ydl.extract_info(video_url, download=False)
        return info_dict.get("url", None)

youtube_video_url = "https://www.youtube.com/watch?v=MUgU-gUOgos&pp=ygUUZ29yZSBzY2VuZXMgaW4gZ2FtZXM%3D"
stream_url = get_youtube_stream_url(youtube_video_url)

cap = cv2.VideoCapture(stream_url)

if not cap.isOpened():
    print("Error: Could not open input video stream.")
    exit()

# Get video properties
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps1 = cap.get(cv2.CAP_PROP_FPS)

# Print video properties for debugging
print(f"Frame Width: {frame_width}, Frame Height: {frame_height}, FPS: {fps1}")

# Video output path
output_video_path = "/content/youtube_original.avi"

# Define the codec and create a VideoWriter object
fourcc = cv2.VideoWriter_fourcc(*'MJPG')  # Use MJPG for .avi format
fps2 = 30
out = cv2.VideoWriter(output_video_path, fourcc, fps2, (frame_width, frame_height))

# Check if the VideoWriter was initialized correctly
if not out.isOpened():
    print("Error: Could not open output video file for writing.")
    cap.release()
    exit()

# Pixelation function
def pixelate_region(frame, x, y, w, h):
    roi = frame[y:y+h, x:x+w]
    roi_small = cv2.resize(roi, (10, 10), interpolation=cv2.INTER_LINEAR)
    roi_pixelated = cv2.resize(roi_small, (w, h), interpolation=cv2.INTER_NEAREST)
    frame[y:y+h, x:x+w] = roi_pixelated

# Start processing the video frame by frame
frame_counter = 0
fps_avg = []  # List to store per-frame FPS
total_processing_time = 0

start_time = time.time()  # Start the timer for total processing

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        print("End of video or cannot read the frame.")
        break

    frame_counter += 1
    frame_start_time = time.time()  # Start timer for each frame

    height, width, channels = frame.shape

    # Prepare the frame for YOLOv3 detection
    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)
    net.setInput(blob)
    outputs = net.forward(output_layers)

    # Get the bounding boxes for detected objects (e.g., person)
    boxes = []
    confidences = []
    class_ids = []
    for output in outputs:
        for detection in output:
            scores = detection[5:]
            class_id = np.argmax(scores)
            confidence = scores[class_id]
            if confidence > 0.5:  # confidence threshold
                center_x = int(detection[0] * width)
                center_y = int(detection[1] * height)
                w = int(detection[2] * width)
                h = int(detection[3] * height)
                x = int(center_x - w / 2)
                y = int(center_y - h / 2)

                # Ensure the bounding box is within the frame
                x = max(0, min(x, width - 1))
                y = max(0, min(y, height - 1))
                w = min(w, width - x)
                h = min(h, height - y)

                boxes.append([x, y, w, h])
                confidences.append(float(confidence))
                class_ids.append(class_id)

    # Non-maximum suppression to eliminate overlapping boxes
    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)

    # Check if indices is not empty
    if len(indices) > 0:
        for i in indices:
            # If indices is an array of lists, unpack; otherwise, just use the scalar
            if isinstance(i, (list, np.ndarray)):
                i = i[0]
            box = boxes[i]
            x, y, w, h = box

            # Prepare the region for pixelation using YOLOv8 model
            roi = frame[y:y+h, x:x+w]
            prediction = model.predict(roi, confidence=40, overlap=30).json()
            print(f"Predictions for frame {frame_counter}: {prediction}")

            # Check and process predictions for pixelation
            if 'predictions' in prediction:
                for pred in prediction['predictions']:
                    if pred['class'] == "pixelate":  # Adjust this condition based on your model's output
                        pixelate_region(frame, x, y, w, h)

    # Write the processed frame to the output video
    out.write(frame)

    # Measure time taken for the frame and calculate FPS for the frame
    frame_end_time = time.time()
    frame_processing_time = frame_end_time - frame_start_time
    total_processing_time += frame_processing_time
    fps = 1 / frame_processing_time
    fps_avg.append(fps)

    # Print frame processing details (optional)
    print(f"Processed frame {frame_counter}, Time: {frame_processing_time:.4f} s, FPS: {fps:.2f}")

# Release video capture and writer objects
cap.release()
out.release()

# Print final performance results
print(f"\n--- Video Processing Complete ---")
print(f"Total frames processed: {frame_counter}")
print(f"Total processing time: {total_processing_time:.2f} seconds")
print(f"Average FPS: {sum(fps_avg) / len(fps_avg) if fps_avg else 0:.2f}")

import cv2
import numpy as np
import time
import yt_dlp
from roboflow import Roboflow

# Ensure correct paths for YOLOv3 weights and config
weights_path = "/content/yolov3.weights"
config_path = "/content/yolov3.cfg"

# Load YOLOv3 model
net = cv2.dnn.readNet(weights_path, config_path)
layer_names = net.getLayerNames()
output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]

# Initialize Roboflow workspace and project
rf = Roboflow(api_key="ZnJzXaB86Tol6zubMIPd")  # Insert your API key
project = rf.workspace().project("pixelation_mk")  # Insert your project ID or endpoint
model = project.version(8).model  # Replace with the correct version number

# Function to get YouTube stream URL using yt-dlp
def get_youtube_stream_url(video_url):
    ydl_opts = {
        'format': 'best[ext=mp4]/best',  # Select the best video format with mp4 extension
        'noplaylist': True,              # Ensure only a single video is downloaded
        'cookiefile': None               # Disable cookies to avoid the cookie file issue
    }

    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info_dict = ydl.extract_info(video_url, download=False)
        return info_dict.get("url", None)

# Provide YouTube video URL
youtube_video_url = "https://www.youtube.com/watch?v=MUgU-gUOgos&pp=ygUUZ29yZSBzY2VuZXMgaW4gZ2FtZXM%3D"
stream_url = get_youtube_stream_url(youtube_video_url)

print(f"Stream URL: {stream_url}")

# Check if the stream URL is valid
if stream_url is None:
    print("Error: Could not get YouTube stream URL.")
    exit()

# Attempt to open the stream using OpenCV
cap = cv2.VideoCapture(stream_url)

if not cap.isOpened():
    print("Error: Could not open input video stream.")
    exit()

# Get video properties
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps1 = cap.get(cv2.CAP_PROP_FPS)

# Print video properties for debugging
print(f"Frame Width: {frame_width}, Frame Height: {frame_height}, FPS: {fps1}")

# Video output path
output_video_path = "/content/youtube_original.avi"

# Define the codec and create a VideoWriter object
fourcc = cv2.VideoWriter_fourcc(*'MJPG')  # Use MJPG for .avi format
fps2 = 30
out = cv2.VideoWriter(output_video_path, fourcc, fps2, (frame_width, frame_height))

# Check if the VideoWriter was initialized correctly
if not out.isOpened():
    print("Error: Could not open output video file for writing.")
    cap.release()
    exit()

# Pixelation function
def pixelate_region(frame, x, y, w, h):
    roi = frame[y:y+h, x:x+w]
    roi_small = cv2.resize(roi, (10, 10), interpolation=cv2.INTER_LINEAR)
    roi_pixelated = cv2.resize(roi_small, (w, h), interpolation=cv2.INTER_NEAREST)
    frame[y:y+h, x:x+w] = roi_pixelated

# Start processing the video frame by frame
frame_counter = 0
fps_avg = []  # List to store per-frame FPS
total_processing_time = 0

start_time = time.time()  # Start the timer for total processing

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        print("End of video or cannot read the frame.")
        break

    frame_counter += 1
    frame_start_time = time.time()  # Start timer for each frame

    height, width, channels = frame.shape

    # Prepare the frame for YOLOv3 detection
    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)
    net.setInput(blob)
    outputs = net.forward(output_layers)

    # Get the bounding boxes for detected objects (e.g., person)
    boxes = []
    confidences = []
    class_ids = []
    for output in outputs:
        for detection in output:
            scores = detection[5:]
            class_id = np.argmax(scores)
            confidence = scores[class_id]
            if confidence > 0.5:  # confidence threshold
                center_x = int(detection[0] * width)
                center_y = int(detection[1] * height)
                w = int(detection[2] * width)
                h = int(detection[3] * height)
                x = int(center_x - w / 2)
                y = int(center_y - h / 2)

                # Ensure the bounding box is within the frame
                x = max(0, min(x, width - 1))
                y = max(0, min(y, height - 1))
                w = min(w, width - x)
                h = min(h, height - y)

                boxes.append([x, y, w, h])
                confidences.append(float(confidence))
                class_ids.append(class_id)

    # Non-maximum suppression to eliminate overlapping boxes
    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)

    # Check if indices is not empty
    if len(indices) > 0:
        for i in indices:
            if isinstance(i, (list, np.ndarray)):
                i = i[0]
            box = boxes[i]
            x, y, w, h = box

            # Prepare the region for pixelation using YOLOv8 model
            roi = frame[y:y+h, x:x+w]
            prediction = model.predict(roi, confidence=40, overlap=30).json()
            print(f"Predictions for frame {frame_counter}: {prediction}")

            # Check and process predictions for pixelation
            if 'predictions' in prediction:
                for pred in prediction['predictions']:
                    if pred['class'] == "pixelate":  # Adjust this condition based on your model's output
                        pixelate_region(frame, x, y, w, h)

    # Write the processed frame to the output video
    out.write(frame)

    # Measure time taken for the frame and calculate FPS for the frame
    frame_end_time = time.time()
    frame_processing_time = frame_end_time - frame_start_time
    total_processing_time += frame_processing_time
    fps = 1 / frame_processing_time
    fps_avg.append(fps)

    # Print frame processing details (optional)
    print(f"Processed frame {frame_counter}, Time: {frame_processing_time:.4f} s, FPS: {fps:.2f}")

# Release video capture and writer objects
cap.release()
out.release()

# Print final performance results
print(f"\n--- Video Processing Complete ---")
print(f"Total frames processed: {frame_counter}")
print(f"Total processing time: {total_processing_time:.2f} seconds")
print(f"Average FPS: {sum(fps_avg) / len(fps_avg) if fps_avg else 0:.2f}")

!pip install gradio
!pip install yt-dlp
!pip install opencv-python

import cv2
import gradio as gr
import yt_dlp
from roboflow import Roboflow
import numpy as np

# Initialize Roboflow workspace and model (you need to replace these with your actual API key and project details)
rf = Roboflow(api_key="ZnJzXaB86Tol6zubMIPd")
project = rf.workspace().project("pixelation_mk")
model = project.version(8).model  # Replace with your actual model version

# Define YOLOv3 model paths
weights_path = "/content/yolov3.weights"
config_path = "/content/yolov3.cfg"
net = cv2.dnn.readNet(weights_path, config_path)
layer_names = net.getLayerNames()
output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]

# Function to get YouTube stream URL using yt-dlp
def get_youtube_stream_url(video_url):
    ydl_opts = {
        'format': 'best[ext=mp4]/best',  # Select best format (mp4)
        'noplaylist': True,              # Avoid playlist extraction
    }

    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info_dict = ydl.extract_info(video_url, download=False)
        return info_dict.get("url", None)

# Pixelation function
def pixelate_region(frame, x, y, w, h):
    roi = frame[y:y+h, x:x+w]
    roi_small = cv2.resize(roi, (10, 10), interpolation=cv2.INTER_LINEAR)
    roi_pixelated = cv2.resize(roi_small, (w, h), interpolation=cv2.INTER_NEAREST)
    frame[y:y+h, x:x+w] = roi_pixelated
    return frame

# Process frame using YOLOv3 and Roboflow model
def process_frame(frame):
    # Prepare frame for YOLO
    height, width, channels = frame.shape
    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)
    net.setInput(blob)
    outputs = net.forward(output_layers)

    # Detect objects (e.g., "person" or others that need pixelation)
    boxes, confidences, class_ids = [], [], []
    for output in outputs:
        for detection in output:
            scores = detection[5:]
            class_id = np.argmax(scores)
            confidence = scores[class_id]
            if confidence > 0.5:
                center_x = int(detection[0] * width)
                center_y = int(detection[1] * height)
                w = int(detection[2] * width)
                h = int(detection[3] * height)
                x = int(center_x - w / 2)
                y = int(center_y - h / 2)
                boxes.append([x, y, w, h])
                confidences.append(float(confidence))
                class_ids.append(class_id)

    # Perform NMS (Non-maximum Suppression)
    indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)

    if len(indices) > 0:
        for i in indices.flatten():
            box = boxes[i]
            x, y, w, h = box
            frame = pixelate_region(frame, x, y, w, h)

    return frame

# Function to handle different inputs: YouTube URL, Webcam, or File Upload
def process_input(youtube_url=None, webcam_input=None, file_input=None):
    frame = None

    if youtube_url:
        # Fetch YouTube stream and process
        stream_url = get_youtube_stream_url(youtube_url)
        if stream_url:
            cap = cv2.VideoCapture(stream_url)
            ret, frame = cap.read()
            cap.release()

    elif webcam_input:
        # Process the webcam stream
        cap = cv2.VideoCapture(0)  # Webcam device
        ret, frame = cap.read()
        cap.release()

    elif file_input:
        # Read image or video file
        if file_input.name.endswith(('.jpg', '.jpeg', '.png')):
            frame = np.array(file_input)  # Process as an image
        else:
            cap = cv2.VideoCapture(file_input.name)
            ret, frame = cap.read()
            cap.release()

    if frame is not None:
        # Process frame using YOLO and Roboflow model
        frame = process_frame(frame)
        return frame
    else:
        return "No valid input provided"

# Define Gradio interface for the app
iface = gr.Interface(
    fn=process_input,
    inputs=[
        gr.Textbox(label="Enter YouTube Video URL", placeholder="e.g. https://www.youtube.com/watch?v=XXXX"),
        gr.File(label="Upload an Image or Video")  # Handles image or video
    ],

    outputs=gr.Image(type="numpy", label="Processed Output")  # Display output image
)

iface.launch()

!pip install roboflow

!pip install gradio
!pip install yt-dlp
!pip install opencv-python

import gradio as gr
from roboflow import Roboflow, CLIPModel
import tempfile
import os

# Initialize Roboflow
rf = Roboflow(api_key="ZnJzXaB86Tol6zubMIPd")
model = CLIPModel(api_key="ZnJzXaB86Tol6zubMIPd")


def analyze_video(video_file):
    # Save uploaded video to a temporary path
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
        tmp.write(video_file.read())
        tmp_path = tmp.name

    try:
        # Send video to Roboflow CLIP model
        job_id, signed_url, expire_time = model.predict_video(
            tmp_path,
            fps=5,
            prediction_type="batch-video",
        )

        results = model.poll_until_video_results(job_id)
        return str(results)

    except Exception as e:
        return f"Error: {str(e)}"

    finally:
        # Clean up
        os.remove(tmp_path)

# Gradio interface
gr.Interface(
    fn=analyze_video,
    inputs=gr.Video(label="Upload Video"),
    outputs="text",
    title="Roboflow CLIP Video Analyzer",
    description="Upload a video and analyze it using Roboflow's CLIPModel"
).launch()

!pip install inference-cli

!inference server start

!pip install inference-cli

!inference server start

!pip install inference

# Import the InferencePipeline object
from inference import InferencePipeline
import cv2

def my_sink(result, video_frame):
    if result.get("output_image"): # Display an image from the workflow response
        cv2.imshow("Workflow Image", result["output_image"].numpy_image)
        cv2.waitKey(1)
    print(result) # do something with the predictions of each frame


# initialize a pipeline object
pipeline = InferencePipeline.init_with_workflow(
    api_key="ZnJzXaB86Tol6zubMIPd",
    workspace_name="dhrisheta-ms-wigrz",
    workflow_id="small-object-detection-sahi",
    video_reference=0, # Path to video, device id (int, usually 0 for built in webcams), or RTSP stream url
    max_fps=30,
    on_prediction=my_sink
)
pipeline.start() #start the pipeline
pipeline.join() #wait for the pipeline thread to finish

!pip install opencv-python opencv-python-headless numpy

!wget https://pjreddie.com/media/files/yolov3.weights
!wget https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg
!wget https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names

# ‚úÖ Setup
from google.colab.patches import cv2_imshow
import cv2
import numpy as np

# ‚úÖ Load class names
classes = []
with open("coco.names", "r") as f:
    classes = [line.strip() for line in f.readlines()]

# ‚úÖ Load YOLOv3
net = cv2.dnn.readNet("yolov3.weights", "yolov3.cfg")
layer_names = net.getLayerNames()
output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers().flatten()]

# ‚úÖ Pixelation function
def pixelate_image(image, bbox, pixel_size=10):
    x, y, w, h = bbox
    x = max(0, x)
    y = max(0, y)
    roi = image[y:y+h, x:x+w]
    if roi.size == 0:
        return image
    small = cv2.resize(roi, (w // pixel_size, h // pixel_size), interpolation=cv2.INTER_LINEAR)
    pixelated = cv2.resize(small, (w, h), interpolation=cv2.INTER_NEAREST)
    image[y:y+h, x:x+w] = pixelated
    return image

# ‚úÖ Object detection and pixelation
def detect_objects_and_pixelate(frame):
    height, width, _ = frame.shape
    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), swapRB=True, crop=False)
    net.setInput(blob)
    outs = net.forward(output_layers)

    class_ids, confidences, boxes = [], [], []

    for out in outs:
        for detection in out:
            scores = detection[5:]
            class_id = np.argmax(scores)
            confidence = scores[class_id]
            if confidence > 0.5:
                center_x = int(detection[0] * width)
                center_y = int(detection[1] * height)
                w = int(detection[2] * width)
                h = int(detection[3] * height)
                x = int(center_x - w / 2)
                y = int(center_y - h / 2)
                boxes.append([x, y, w, h])
                confidences.append(float(confidence))
                class_ids.append(class_id)

    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)

    if len(indexes) > 0:
        for i in indexes.flatten():
            x, y, w, h = boxes[i]
            frame = pixelate_image(frame, (x, y, w, h))
            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)

    return frame

# ‚úÖ Process video in Colab
def process_video(input_video_path, output_video_path):
    cap = cv2.VideoCapture(input_video_path)
    if not cap.isOpened():
        print("‚ùå Cannot open video.")
        return

    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)

    fourcc = cv2.VideoWriter_fourcc(*'XVID')
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

    frame_count = 0
    print("üîÅ Processing video...")

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        processed_frame = detect_objects_and_pixelate(frame)
        out.write(processed_frame)

        if frame_count % 10 == 0:
            print(f"üì∏ Frame {frame_count}")
            cv2_imshow(processed_frame)

        frame_count += 1

    cap.release()
    out.release()
    print("‚úÖ Done! Saved to:", output_video_path)

# ‚úÖ Set your file paths
input_video = "/content/MK9.mp4"
output_video = "/content/output_pixelated.avi"

process_video(input_video, output_video)

!pip install streamlit opencv-python-headless numpy
!streamlit run app.py

import streamlit as st
import cv2
import numpy as np
import tempfile
import os
from datetime import datetime

# Load YOLO
@st.cache_resource
def load_yolo():
    classes = []
    with open("coco.names", "r") as f:
        classes = [line.strip() for line in f.readlines()]
    net = cv2.dnn.readNet("yolov3.weights", "yolov3.cfg")
    layer_names = net.getLayerNames()
    output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers().flatten()]
    return net, output_layers, classes

net, output_layers, classes = load_yolo()

# Pixelation
def pixelate_image(image, bbox, pixel_size=10):
    x, y, w, h = bbox
    roi = image[y:y+h, x:x+w]
    if roi.size == 0:
        return image
    small = cv2.resize(roi, (w // pixel_size, h // pixel_size), interpolation=cv2.INTER_LINEAR)
    pixelated = cv2.resize(small, (w, h), interpolation=cv2.INTER_NEAREST)
    image[y:y+h, x:x+w] = pixelated
    return image

# Detection + pixelation
def detect_objects_and_pixelate(frame):
    height, width, _ = frame.shape
    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), swapRB=True, crop=False)
    net.setInput(blob)
    outs = net.forward(output_layers)

    class_ids, confidences, boxes = [], [], []

    for out in outs:
        for detection in out:
            scores = detection[5:]
            class_id = np.argmax(scores)
            confidence = scores[class_id]
            if confidence > 0.5:
                center_x = int(detection[0] * width)
                center_y = int(detection[1] * height)
                w = int(detection[2] * width)
                h = int(detection[3] * height)
                x = int(center_x - w / 2)
                y = int(center_y - h / 2)
                boxes.append([x, y, w, h])
                confidences.append(float(confidence))
                class_ids.append(class_id)

    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)

    if len(indexes) > 0:
        for i in indexes.flatten():
            x, y, w, h = boxes[i]
            frame = pixelate_image(frame, (x, y, w, h))
    return frame

# Process uploaded video
def process_video(input_path, output_path, show_every=30):
    cap = cv2.VideoCapture(input_path)
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    fourcc = cv2.VideoWriter_fourcc(*'XVID')
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    progress = st.progress(0)
    preview_frame = st.empty()

    frame_count = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        processed_frame = detect_objects_and_pixelate(frame)
        out.write(processed_frame)

        if frame_count % show_every == 0:
            preview_frame.image(cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB), caption=f"Frame {frame_count}", channels="RGB")

        frame_count += 1
        progress.progress(min(1.0, frame_count / total_frames))

    cap.release()
    out.release()

    st.success("‚úÖ Processing complete!")

# ================= STREAMLIT UI =======================
st.set_page_config(page_title="YOLO Pixelation Video Processor", layout="wide")

st.title("üé• YOLO-Powered Offensive Object Pixelator")
st.markdown("Upload a video and automatically pixelate offensive objects (guns, knives, blood, etc.) using YOLOv3.")

uploaded_file = st.file_uploader("üì§ Upload your video file", type=["mp4", "avi", "mov"])
if uploaded_file:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as temp_input:
        temp_input.write(uploaded_file.read())
        input_path = temp_input.name

    output_filename = f"output_{datetime.now().strftime('%H%M%S')}.avi"
    output_path = os.path.join(tempfile.gettempdir(), output_filename)

    st.info("‚è≥ Starting processing...")
    process_video(input_path, output_path)

    with open(output_path, 'rb') as f:
        st.download_button("üì• Download Output Video", f, file_name="pixelated_output.avi", mime='video/avi')

!streamlit run vista.py

# ‚úÖ Install requirements
!pip install gradio opencv-python-headless

# ‚úÖ Imports
import gradio as gr
import cv2
import numpy as np
import tempfile
import os

# ‚úÖ Load class names
with open("coco.names", "r") as f:
    classes = [line.strip() for line in f.readlines()]

# ‚úÖ Load YOLOv3
net = cv2.dnn.readNet("yolov3.weights", "yolov3.cfg")
layer_names = net.getLayerNames()
output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers().flatten()]

# ‚úÖ Pixelate Function
def pixelate_image(image, bbox, pixel_size=10):
    x, y, w, h = bbox
    x = max(0, x)
    y = max(0, y)
    roi = image[y:y+h, x:x+w]
    if roi.size == 0:
        return image
    small = cv2.resize(roi, (max(1, w // pixel_size), max(1, h // pixel_size)), interpolation=cv2.INTER_LINEAR)
    pixelated = cv2.resize(small, (w, h), interpolation=cv2.INTER_NEAREST)
    image[y:y+h, x:x+w] = pixelated
    return image

# ‚úÖ Detect + Pixelate Objects
def detect_and_pixelate(frame):
    height, width, _ = frame.shape
    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)
    net.setInput(blob)
    outs = net.forward(output_layers)

    boxes, confidences, class_ids = [], [], []

    for out in outs:
        for det in out:
            scores = det[5:]
            class_id = np.argmax(scores)
            confidence = scores[class_id]
            if confidence > 0.5:
                cx, cy, w, h = int(det[0]*width), int(det[1]*height), int(det[2]*width), int(det[3]*height)
                x = int(cx - w/2)
                y = int(cy - h/2)
                boxes.append([x, y, w, h])
                confidences.append(float(confidence))
                class_ids.append(class_id)

    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)

    for i in indexes.flatten():
        x, y, w, h = boxes[i]
        frame = pixelate_image(frame, (x, y, w, h))

    return frame

# ‚úÖ Process Video
def process_video(video_path):
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    temp_output = tempfile.NamedTemporaryFile(suffix=".avi", delete=False)
    output_path = temp_output.name

    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*"XVID"), fps, (width, height))
    frame_count = 0

    while True:
        ret, frame = cap.read()
        if not ret:
            break
        pixelated_frame = detect_and_pixelate(frame)
        out.write(pixelated_frame)
        frame_count += 1
        if frame_count % 30 == 0:
            print(f"üì∏ Processed {frame_count} frames...")

    cap.release()
    out.release()

    return output_path

# ‚úÖ Gradio Interface
gr_interface = gr.Interface(
    fn=process_video,
    inputs=gr.Video(label="üì§ Upload Your Video"),
    outputs=gr.Video(label="üì• Pixelated Output Video"),
    title="üî≤ Real-Time Object Pixelator (YOLOv3 + Gradio)",
    description="Upload any video. Offensive/undesired objects will be pixelated using YOLOv3."
)

# ‚úÖ Launch
gr_interface.launch()

!pip install gradio --quiet

import gradio as gr
import cv2
import tempfile

def process_video(video_file):
    cap = cv2.VideoCapture(video_file.name)

    # Output video setup
    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps    = cap.get(cv2.CAP_PROP_FPS)
    temp_out = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')

    out = cv2.VideoWriter(temp_out.name, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))

    while True:
        ret, frame = cap.read()
        if not ret:
            break
        # Just converting to grayscale and back as placeholder
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        final = cv2.cvtColor(gray, cv2.COLOR_GRAY2BGR)
        out.write(final)

    cap.release()
    out.release()
    return temp_out.name

gr.Interface(
    fn=process_video,
    inputs=gr.File(label="Upload MP4"),
    outputs=gr.Video(label="Processed Video"),
    title="üé• Simple Video Processor",
).launch(pwa=True)

import cv2
import numpy as np
import tempfile
import shutil
import os
from google.colab.patches import cv2_imshow
from IPython.display import HTML
from base64 import b64encode

# Load YOLOv3
classes = []
with open("coco.names", "r") as f:
    classes = [line.strip() for line in f.readlines()]

net = cv2.dnn.readNet("yolov3.weights", "yolov3.cfg")
layer_names = net.getLayerNames()
output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers().flatten()]

# Pixelation Function
def pixelate_image(image, bbox, pixel_size=10):
    x, y, w, h = bbox
    x = max(0, x)
    y = max(0, y)
    h = min(h, image.shape[0] - y)
    w = min(w, image.shape[1] - x)

    roi = image[y:y+h, x:x+w]
    if roi.size == 0:
        return image

    # Avoid zero dimension during resizing
    new_w = max(1, w // pixel_size)
    new_h = max(1, h // pixel_size)

    # Resize and restore
    small = cv2.resize(roi, (new_w, new_h), interpolation=cv2.INTER_LINEAR)
    pixelated = cv2.resize(small, (roi.shape[1], roi.shape[0]), interpolation=cv2.INTER_NEAREST)

    image[y:y+h, x:x+w] = pixelated
    return image

# Detection + Pixelation
def detect_objects_and_pixelate(frame):
    height, width, _ = frame.shape
    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0,0,0), swapRB=True, crop=False)
    net.setInput(blob)
    outs = net.forward(output_layers)

    class_ids, confidences, boxes = [], [], []

    for out in outs:
        for detection in out:
            scores = detection[5:]
            class_id = np.argmax(scores)
            confidence = scores[class_id]
            if confidence > 0.5:
                center_x = int(detection[0] * width)
                center_y = int(detection[1] * height)
                w = int(detection[2] * width)
                h = int(detection[3] * height)
                x = int(center_x - w / 2)
                y = int(center_y - h / 2)
                boxes.append([x, y, w, h])
                confidences.append(float(confidence))
                class_ids.append(class_id)

    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)

    if len(indexes) > 0:
        for i in indexes.flatten():
            x, y, w, h = boxes[i]
            frame = pixelate_image(frame, (x, y, w, h))
            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)

    return frame

# Upload video
from google.colab import files
uploaded = files.upload()

# Assume only one file uploaded
input_video_path = list(uploaded.keys())[0]

# Prepare output video path
output_path = "output_pixelated.mp4"
cap = cv2.VideoCapture(input_video_path)

if not cap.isOpened():
    print("‚ùå Cannot open uploaded video.")

width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS)

fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
print(f"üîÑ Processing {frame_count} frames...")

count = 0
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    processed_frame = detect_objects_and_pixelate(frame)
    out.write(processed_frame)
    count += 1

    if count % 10 == 0:
        print(f"‚úÖ Processed {count}/{frame_count} frames")

cap.release()
out.release()

print("‚úÖ Done! Displaying final video...")

# Display output video
mp4 = open(output_path, 'rb').read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()
HTML(f"""
<video width=600 controls>
    <source src="{data_url}" type="video/mp4">
</video>
""")

!git config --global user.name "Dhrisheta"
!git config --global user.email "dhrishetadhrisheta@gmail.com"

!git clone https://github.com/Dhrisheta/vista-dataminds

!mv /content/vista.py/content/vista-dataminds/

!ls